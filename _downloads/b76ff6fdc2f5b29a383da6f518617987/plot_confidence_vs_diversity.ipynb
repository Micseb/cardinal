{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nLowest confidence vs. KMeans sampling\n=====================================\n\nIn this example, we show the usefulness of diversity-based approaches using a\ntoy example where a very unlucky initialization makes lowest confidence\napproach underperform.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Those are the necessary imports and initialiaztion\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\nfrom matplotlib.patches import Polygon\nimport numpy as np\n\nfrom sklearn.datasets.samples_generator import make_blobs\nfrom sklearn.svm import SVC\n\nfrom cardinal.uncertainty import ConfidenceSampler\nfrom cardinal.clustering import KMeansSampler\nfrom cardinal.batch import RankedBatchSampler\nfrom cardinal.random import RandomSampler\n\nnp.random.seed(7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Parameters of our experiment:\n* _n_ is the number of points in the sumulated data\n* _batch_size_ is the number of samples that will be annotated and added to\n  the training set at each iteration\n* _n_iter_ is the number of iterations in our simulation\n\nWe simulate data where the samples of one of the class are scattered in 3\nblobs, one of them being far away from the two others. We also select an\ninitialization index where no sample from the far-away sample is initially\nselected. This will force the decision boundary to stay far from that cluster\nand thus \"trick\" the lowest confidence method.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n = 28\nbatch_size = 4\nn_iter = 5\n\nX, y = make_blobs(n_samples=n, centers=[(1, 0), (0, 1), (2, 2), (4, 0)],\n                  random_state=0, cluster_std=0.2)\n    \n# We select samples in clusters 0, 1 and 2. Cluster 3 will be ignored by uncertainty sampling\ninit_idx = [i for j in range(3) for i in np.where(y == j)[0][:2]]\ny[y > 1] = 1\n\nmodel = SVC(kernel='linear', C=1E10, probability=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This helper function plots our simulated points in red and blue. The one that\nare not in the training set are faded. We also plot the linear separation\nestimated by the SVM.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def plot(a, b, score, selected):\n\n    plt.xlabel('Accuracy {}%'.format(int(score * 100)), fontsize=10)\n\n    l_to_c = {0: 'tomato', 1:'royalblue'}\n\n    f = (lambda x: a * x + b)\n    x1, x2 = (np.min(X[:, 0]), np.max(X[:, 0]))\n    y1, y2 = (np.min(X[:, 1]), np.max(X[:, 1]))\n\n    # We compute the intersection of the line with the rectange\n    p1, p2 = (x1, a * x1 + b), ((y1 - b) / a, y1)\n    p3, p4 = (x2, a * x2 + b), ((y2 - b) / a, y2)\n    p1, p2, p3, p4 = sorted([p1, p2, p3, p4])\n\n    corners = [(x1, y1), (x1, y2), (x2, y2), (x2, y1)]\n    dists = [f(x) - y for x, y in corners]\n    while dists[0] > 0 or dists[-1] < 0:\n        dists.append(dists.pop(0))\n        corners.append(corners.pop(0))\n    first_pos = next(i for i, x in enumerate(dists) if x > 0)\n    plt.gca().add_patch(Polygon([p3, p2] + corners[:first_pos], joinstyle='round',\n        facecolor=l_to_c[model.predict([corners[0]])[0]], alpha=0.2))\n    plt.gca().add_patch(Polygon([p2, p3] + corners[first_pos:], joinstyle='round',\n        facecolor=l_to_c[model.predict([corners[-1]])[0]], alpha=0.2))\n\n    #plt.fill_between([p1[0], p4[0]], [p1[1], p4[1]], [p4[1], p4[1]],\n    #                 color=l_to_c[model.predict([(p1[0], p4[1])])[0]], alpha=0.1)\n    #plt.fill_between([p1[0], p4[0]], [p1[1], p4[1]], [p1[1], p1[1]],\n    #                 color=l_to_c[model.predict([(p4[0], p1[1])])[0]], alpha=0.1)\n   \n    # Plot not selected first in low alpha, then selected\n    for l, s in [(0, False), (1, False), (0, True), (1, True)]:\n        alpha = 1. if s else 0.3\n        mask = np.logical_and(selected == s, l == y)\n        plt.scatter(X[mask, 0], X[mask, 1], c=l_to_c[l], alpha=alpha)\n        \n    # Plot the separation margin of the SVM\n    plt.plot(*zip(p2, p3), c='purple')\n    eps = 0.1\n    plt.gca().set_xlim(x1 - eps, x2 + eps)\n    plt.gca().set_ylim(y1 - eps, y2 + eps)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Core active learning experiment\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nAs presented in the introduction, this loop represents the active learning\nexperiment. At each iteration, the model is learned on all labeled data to\nmeasure its performance. Then, the model is inspected to find out the samples\non which its confidence is low. This is done through cardinal samplers.\n\nIn this experiment, we see that lowest confidence will explore the far-away\ncluster only once all other samples have been labeled. KMeans uses a more\nexploratory approach and select items in this cluster right away.\nIt is worth noticing that random sampling also have good exploration\nproperties.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "samplers = [\n    ('Lowest confidence', ConfidenceSampler(model, batch_size)),\n    ('KMeans', KMeansSampler(batch_size)),\n    ('WKMeans', KMeansSampler(batch_size)),\n    ('Batch', RankedBatchSampler(batch_size)),\n    ('Random', RandomSampler(batch_size))\n]\n\nplt.figure(figsize=(10, 10))\n\nfor i, (sampler_name, sampler) in enumerate(samplers):\n    mask = np.zeros(n, dtype=bool)\n    indices = np.arange(n)\n    mask[init_idx] = True\n\n    for j in range(n_iter):\n        model.fit(X[mask], y[mask])\n        sampler.fit(X[mask], y[mask])\n        w = model.coef_[0]\n        \n        plt.subplot(len(samplers), n_iter, i * n_iter + j + 1)\n\n        if sampler_name == 'Batch':\n            # This is an SSL method that requires \n            weights = ConfidenceSampler(model, batch_size).score_samples(X)\n            weights[mask] = -1\n            selected = sampler.select_samples(X, samples_weights=weights)\n            mask[selected] = True\n        elif sampler_name == 'WKmeans':\n            weights = ConfidenceSampler(model, batch_size).score_samples(X[~mask])\n            selected = sampler.select_samples(X[~mask], samples_weights=weights)\n            mask[indices[~mask][selected]] = True\n        else:\n            selected = sampler.select_samples(X[~mask])\n            mask[indices[~mask][selected]] = True\n\n        if j == 0:\n            plt.ylabel(sampler_name)\n        plt.axis('tight')\n        plt.gca().set_xticks(())\n        plt.gca().set_yticks(())\n        if i == 0:\n            plt.gca().set_title('Iteration {}'.format(j), fontsize=10)\n\n        plot(-w[0] / w[1], - model.intercept_[0] / w[1], model.score(X, y), mask.copy())\n\nplt.tight_layout()\nplt.subplots_adjust(top=0.86)\nplt.gcf().suptitle('Classification accuracy of random and uncertainty active learning on simulated data', fontsize=12)\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}