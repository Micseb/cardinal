{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nActive Learning on Digit Recognition and Metrics\n================================================\n\nIn this example, we run an experiment on real data and show how active\nlearning can be monitored, given that in real life scenario there is no access\nto the ground truth of the test set. Based on these metrics, we\nidentify two phases during this active learning experiment and define\na custom query sampler that takes advantage of this.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Those are the necessary imports and initializations\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\nimport numpy as np\n\nfrom sklearn.datasets import load_digits\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import pairwise_distances\n\nfrom cardinal.uncertainty import ConfidenceSampler\nfrom cardinal.clustering import KMeansSampler\nfrom cardinal.random import RandomSampler\nfrom cardinal.plotting import plot_confidence_interval\nfrom cardinal.base import BaseQuerySampler\n\nnp.random.seed(7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The parameters of this experiment are:  \n\n* `batch_size` is the number of samples that will be annotated and added to\n  the training set at each iteration,\n* `n_iter` is the number of iterations in our simulation\n\nWe use the digits dataset and a RandomForestClassifier as model.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "batch_size = 20\nn_iter = 20\n\nX, y = load_digits(return_X_y=True)\nX /= 255.\n\nmodel = RandomForestClassifier()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Experimental Metrics\n--------------------\n\nWe define a first metric based on contradictions. It has been observed that\nthe number of samples on which the model changes his prediction from one\niteration to the other is correlated to the improvement of accuracy. We\nwant to verify this. Since the number of label prediction changes can be\ncoarse, we use the absolute difference in prediction probabilities.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def compute_contradiction(previous_proba, current_proba, weights=None):\n    print(np.average(np.abs(current_proba - previous_proba).mean(axis=1), weights=None))\n    print(np.average(np.abs(current_proba - previous_proba).mean(axis=1), weights=weights))\n    return np.average(np.abs(current_proba - previous_proba).mean(axis=1), weights=weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We define a second metric based on the distance between already labeled\nsamples and our test set. The goal of this metric is measure how well our\ntest set has been explored by our query sampling method so far. We expect\nuncertainty sampling to explore the sample space located *nearby* the\ndecision boundary and show poor exploration property.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def compute_exploration(X_selected, X_test):\n    return pairwise_distances(X_selected, X_test).mean()\n\n\ndef compute_exploration_2(X_selected, X_test):\n    return pairwise_distances(X_selected, X_test).min(axis=0).mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A New Custom Sampler\n--------------------\n\nLet's imagine what an ideal query sampler could look like by making the\nbest of the samplers we already know.  \nUncertainty sampling is certainly an appealing method but in a previous\nexample we have shown that it lacks exploration and can stay stuck in a\nlocal minimum for a while. On the other hand, K-Means sampling explore the\nspace well but will probably fail at fine tuning our prediction model. In\nthat context, it seems reasonable to first explore the sample\nspace, say by using a KMeansSampler, and at some point shift to\nan exploitation mode where we fine tune our model using UncertaintySampler.\nWe define an Adaptive Sampler that does exactly this.\n\nAs a heuristic, let us say that we keep exploring until we have explored 10%\nof our test set.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class AdaptiveQuerySampler(BaseQuerySampler):\n    def __init__(self, exploration_sampler, exploitation_sampler):\n        self.exploration_sampler = exploration_sampler\n        self.exploitation_sampler = exploitation_sampler\n        self._X_train_size = None\n    \n    def fit(self, X_train, y_train):\n        self._X_train_size = X_train.shape[0]\n        self.exploration_sampler.fit(X_train, y_train)\n        self.exploitation_sampler.fit(X_train, y_train)\n        return self\n    \n    def select_samples(self, X):\n        if self._X_train_size <= 50:\n            return self.exploration_sampler.select_samples(X)\n        else:\n            return self.exploitation_sampler.select_samples(X)\n\n\nadaptive_sampler = AdaptiveQuerySampler(\n    KMeansSampler(batch_size),  # Exploration\n    ConfidenceSampler(model, batch_size)  # Exploitation\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Core Active Learning Experiment\n-------------------------------\n\nWe now perform the experiment. We compare our adaptive model to random,\npure exploration, and pure exploitation. We also monitor the metrics\ndefined above.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "samplers = [\n    ('Adaptive', adaptive_sampler),\n    ('Lowest confidence', ConfidenceSampler(model, batch_size)),\n    ('KMeans', KMeansSampler(batch_size)),\n    ('Random', RandomSampler(batch_size)),\n]\n\nfigure_accuracies = plt.figure().number\nfigure_contradictions = plt.figure().number\nfigure_explorations = plt.figure().number\n\nfor i, (sampler_name, sampler) in enumerate(samplers):\n    \n    all_accuracies = []\n    all_contradictions = []\n    all_explorations = []\n\n    for k in range(10):\n        X_train, X_test, y_train, y_test = \\\n            train_test_split(X, y, test_size=500, random_state=k)\n\n        accuracies = []\n        contradictions = []\n        explorations = []\n\n        previous_proba = None\n\n        # For simplicity, we start with one sample of each class\n        _, selected = np.unique(y_train, return_index=True)\n\n        # We use binary masks to simplify some operations\n        mask = np.zeros(X_train.shape[0], dtype=bool)\n        indices = np.arange(X_train.shape[0])\n        mask[selected] = True\n\n        # The classic active learning loop\n        for j in range(n_iter):\n            model.fit(X_train[mask], y_train[mask])\n\n            # Record metrics\n            accuracies.append(model.score(X_test, y_test))\n            explorations.append(compute_exploration(X_train[mask], X_test))\n\n            # Contradictions depend on the previous iteration\n            current_proba = model.predict_proba(X_test)\n\n\n            ddd = 1/pairwise_distances(X_train[mask], X_test).mean(axis=0)\n            print(ddd.min(), ddd.max())\n\n\n            if previous_proba is not None:\n                contradictions.append(compute_contradiction(\n                    previous_proba, current_proba, weights=ddd ** 2))\n            previous_proba = current_proba\n\n            sampler.fit(X_train[mask], y_train[mask])\n            selected = sampler.select_samples(X_train[~mask])\n            mask[indices[~mask][selected]] = True\n\n        all_accuracies.append(accuracies)\n        all_explorations.append(explorations)\n        all_contradictions.append(contradictions)\n    \n    x_data = np.arange(10, batch_size * (n_iter - 1) + 11, batch_size)\n\n    plt.figure(figure_accuracies)\n    plot_confidence_interval(x_data, all_accuracies, label=sampler_name)\n\n    plt.figure(figure_contradictions)\n    plot_confidence_interval(x_data[1:], all_contradictions,\n                             label=sampler_name)\n\n    plt.figure(figure_explorations)\n    plot_confidence_interval(x_data, all_explorations, label=sampler_name)\n\nplt.figure(figure_accuracies)\nplt.xlabel('Labeled samples')\nplt.ylabel('Accuracy')\nplt.gca().axvline(50, color='r')\nplt.legend()\nplt.tight_layout()\n\nplt.figure(figure_contradictions)\nplt.xlabel('Labeled samples')\nplt.ylabel('Contradictions')\nplt.gca().axvline(50, color='r')\nplt.legend()\nplt.tight_layout()\n\nplt.figure(figure_explorations)\nplt.xlabel('Labeled samples')\nplt.ylabel('Exploration score')\nplt.gca().axvline(50, color='r')\nplt.legend()\nplt.tight_layout()\n\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Discussion\n----------\n\nAccuracies\n^^^^^^^^^^\n\nIn all the figures above, the vertical red line indicates when the adaptive\nmethod switches from exploration to exploitation.\n\nWe first look at accuracy. As expected, KMeansSampler, a purely exploration\nmethod is the best at the beginning but becomes as performant as random\nwith time. Uncertainty sampling also behaves as expected by starting poorly and\nthen becoming the best method.\n\nOur AdaptiveQuerySampler combines the performance of both approaches to have the\nbest performance!\n\nContradictions\n^^^^^^^^^^^^^^\n\nWe now want to know if contradictions are a good proxy for performance. We\nobserve that it indeed looks related to the speed (gradient)\nof the accuracy curves. In the end of the experiment in particular,\nuncertainty and adaptive the ones increasing faster and their contradictions\nare also the highest.\n\nExploration Scores\n^^^^^^^^^^^^^^^^^^\n\nThe exploration curve also displays interesting trends. Since those are\ndistance, a good exploration method will have a low score. As expected,\nexploration-based methods have the lowest scores. In particular,\nKMeansSampler starts by decreasing and then goes up. At the same time, its\nperformance starts stalling. This shift happens incidentally at the same\ntime as our adaptive method shifts its method. This is obviously not random!\nThis exploration metric can be used to decide when to change method.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}